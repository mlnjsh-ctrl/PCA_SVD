{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0dec6937",
   "metadata": {},
   "source": [
    "# Formulating PCA as a Distance-Based Optimization Problem\n",
    "\n",
    "## Overview\n",
    "\n",
    "Principal Component Analysis (PCA) is traditionally understood as a method for dimensionality reduction that identifies the directions (principal components) along which the data varies the most. However, PCA can also be formulated as an optimization problem that seeks to find a lower-dimensional subspace minimizing the total reconstruction error, measured by the sum of squared distances between the original data points and their projections onto this subspace.\n",
    "\n",
    "## Mathematical Formulation\n",
    "\n",
    "### Given:\n",
    "\n",
    "A dataset $X \\in \\mathbb{R}^{n \\times p}$ with $n$ observations and $p$ variables. The goal is to reduce the dimensionality from $p$ to $k$ ($k < p$).\n",
    "\n",
    "### Objective:\n",
    "\n",
    "Find a set of orthonormal vectors $U = [u_1, u_2, \\dots, u_k] \\in \\mathbb{R}^{p \\times k}$ that minimizes the total squared reconstruction error.\n",
    "\n",
    "### Optimization Problem:\n",
    "\n",
    "$$\n",
    "\\min_U E = \\sum_{i=1}^{n} \\| x_i - UU^\\top x_i \\|^2\n",
    "$$\n",
    "subject to:\n",
    "$$\n",
    "U^\\top U = I_k\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $x_i$ is the $i$-th data point.\n",
    "- $UU^\\top$ is the projection matrix onto the subspace spanned by $U$.\n",
    "- $I_k$ is the $k \\times k$ identity matrix.\n",
    "- The constraint $U^\\top U = I_k$ ensures that the principal components are orthonormal.\n",
    "\n",
    "## Derivation Steps\n",
    "\n",
    "### 1. Center the Data\n",
    "\n",
    "Subtract the mean from the data to center it around the origin:\n",
    "\n",
    "$$\n",
    "x_i^{centered} = x_i - \\mu, \\quad \\text{where} \\quad \\mu = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n",
    "$$\n",
    "\n",
    "### 2. Define Reconstruction Error\n",
    "\n",
    "The reconstruction error for a single data point is the squared Euclidean distance between the original point and its projection:\n",
    "\n",
    "$$\n",
    "e_i = \\| x_i^{centered} - UU^\\top x_i^{centered} \\|^2\n",
    "$$\n",
    "\n",
    "Total reconstruction error:\n",
    "\n",
    "$$\n",
    "E = \\sum_{i=1}^{n} e_i = \\sum_{i=1}^{n} \\| x_i^{centered} - UU^\\top x_i^{centered} \\|^2\n",
    "$$\n",
    "\n",
    "### 3. Simplify the Error Term\n",
    "\n",
    "Using properties of trace and matrix operations:\n",
    "\n",
    "$$\n",
    "E = \\sum_{i=1}^{n} (x_i^{centered} - UU^\\top x_i^{centered})^\\top (x_i^{centered} - UU^\\top x_i^{centered})\n",
    "$$\n",
    "\n",
    "Simplify using trace notation:\n",
    "\n",
    "$$\n",
    "E = \\text{Tr} \\left( (I - UU^\\top) S (I - UU^\\top)^\\top \\right)\n",
    "$$\n",
    "\n",
    "Where $S$ is the sample covariance matrix:\n",
    "\n",
    "$$\n",
    "S = \\frac{1}{n} \\sum_{i=1}^{n} x_i^{centered} (x_i^{centered})^\\top\n",
    "$$\n",
    "\n",
    "Since $(I - UU^\\top)$ is symmetric and idempotent, the error simplifies to:\n",
    "\n",
    "$$\n",
    "E = \\text{Tr} \\left( (I - UU^\\top) S \\right)\n",
    "$$\n",
    "\n",
    "### 4. Formulate the Optimization Objective\n",
    "\n",
    "The optimization problem becomes:\n",
    "\n",
    "$$\n",
    "\\min_U E = \\text{Tr}(S) - \\text{Tr}(U^\\top S U)\n",
    "$$\n",
    "\n",
    "$\\text{Tr}(S)$ is constant with respect to $U$. Minimizing $E$ is equivalent to maximizing $\\text{Tr}(U^\\top S U)$.\n",
    "\n",
    "### 5. Solve the Optimization Problem\n",
    "\n",
    "The problem reduces to:\n",
    "\n",
    "$$\n",
    "\\max_U \\text{Tr}(U^\\top S U) \\quad \\text{subject to} \\quad U^\\top U = I_k\n",
    "$$\n",
    "\n",
    "This is a well-known problem in linear algebra, and the solution involves:\n",
    "\n",
    "- Finding the eigenvalues $\\lambda_j$ and eigenvectors $u_j$ of $S$, such that $S u_j = \\lambda_j u_j$.\n",
    "- Ordering the eigenvalues in decreasing order $\\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_p$.\n",
    "- Selecting the top $k$ eigenvectors corresponding to the largest $k$ eigenvalues to form $U$.\n",
    "\n",
    "## Intuitive Explanation\n",
    "\n",
    "- **Projection**: Each data point is projected onto a $k$-dimensional subspace spanned by the principal components in $U$.\n",
    "- **Reconstruction**: The projected point $\\hat{x}_i = UU^\\top x_i^{centered}$ is the best approximation of $x_i^{centered}$ within the subspace.\n",
    "- **Error Minimization**: By minimizing the sum of squared distances between $x_i^{centered}$ and $\\hat{x}_i$, we ensure the subspace captures as much information as possible from the original data.\n",
    "\n",
    "## Geometric Interpretation\n",
    "\n",
    "- **Data Variability**: The directions of maximum variability in the data correspond to the directions along which the data points spread out the most.\n",
    "- **Principal Components**: The eigenvectors $u_j$ define these directions.\n",
    "- **Subspace Approximation**: The $k$-dimensional subspace is the best linear approximation of the data in terms of minimizing the squared reconstruction error.\n",
    "\n",
    "## Alternative Formulation Using Singular Value Decomposition (SVD)\n",
    "\n",
    "- **SVD of Centered Data Matrix**: Compute the SVD of the centered data matrix $X_{centered} = U \\Sigma V^\\top$.\n",
    "- **Principal Components**: Columns of $V$ are the principal directions.\n",
    "- **Reconstruction Error**: Minimizing the Frobenius norm $\\| X_{centered} - X_k \\|_F^2$, where $X_k = U_k \\Sigma_k V_k^\\top$ is the rank-$k$ approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94d67be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
